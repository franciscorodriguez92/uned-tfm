%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% EVALUACION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Evaluación y discusión}
\fancyhead[RE]{\textsc{CAPÍTULO} \thechapter. Evaluación}
\label{ch:Evaluacion}

\noindent En el siguiente capítulo se presentan los procedimientos de evaluación así como los resultados de los experimentos realizados. Además, se realiza una comparativa y discusión de los distintos resultados obtenidos. Para evaluar el sistema propuesto, se definen dos experimentos según el procedimiento aplicado para realizar la evaluación del sistema. En el primero, se reservan una parte de los datos para realizar una búsqueda de los hiperparámetros óptimos para cada algoritmo de clasificación mientras que en el segundo se emplean los parámetros por defecto para evitar el sobreajuste. Con estos experimentos, se pretende evaluar el rendimiento del sistema propuesto para detección del machismo en redes sociales. Asimismo, se evaluará el efecto que tiene en el sistema el desbalanceo existente en la clase del conjunto de datos.

\section{Metodología de evaluación}
\label{sec:Met_Eval}

\subsection{Métricas de evaluación}
\label{sec:Col_Eval}

\noindent Para la evaluación de los resultados en clasificación textual o de documentos se utiliza comúnmente la matriz de confusión. Se trata de una herramienta que representa en cada columna el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real. En la siguiente imagen se presenta un esquema de la matriz de confusión:

\begin{center}
	\includegraphics[width=0.5\textwidth]{imagenes/confusion_matrix_1.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Matriz de confusión}	
\end{center}

Esta tabla está formada por verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos. De este modo, si un documento es clasificado por el sistema automático en la misma categoría que la clasificación manual, se considerará como un verdadero positivo o negativo (\textit{True Positive}, TP o \textit{True Negative}, TN), mientras que si el documento es clasificado por el sistema con una categoría diferente, se estará ante un falso negativo o falso positivo (\textit{False Positive}, FP o \textit{False Negative}, FN).Utilizando estos cuatro componentes se calculan las medidas principales para evaluar los resultados:

\begin{itemize}
	\itemsep0em 
	\item Tasa de acierto o exactitud (\textit{accuracy}): representa el porcentaje de aciertos en relación a todos los documentos clasificados.
	\[\textit{Accuracy}=\frac{TP+TN}{TP+FP+TN+FN}\]
	\item Precisión: representa la fracción de asignaciones correctas frente al total de asignaciones positivas realizadas para esa clase. Es decir, realiza una medida de la tasa de acierto para un valor de la clase.
	\[Precision=\frac{TP}{TP+FP}\]
	\item Cobertura (\textit{recall}): representa la fracción de asignaciones positivas respecto al conjunto real de elementos pertenecientes a la clase. Es decir, realiza una medida de la capacidad que tiene el clasificador de detectar elementos de esa clase.
	\[Cobertura=\frac{TP}{TP+FN}\]
	\item Medida-F: combina las medidas de precision y cobertura.
	\[Medida-F=\frac{2xprecisionxcobertura}{precision+cobertura}\]
\end{itemize}


\subsection{Colección de evaluación}
\label{sec:Col_Eval}

\noindent Las colecciones de evaluación son conjuntos de datos etiquetados con información relevante para la tarea para la cual han sido desarrollados. En este caso, las colecciones de evaluación para clasificación de documentos están compuestas por textos, ya sean oraciones, párrafos o documentos completos, de distinta naturaleza y que están etiquetados con categorías concretas. Por ejemplo, para el presente trabajo, existen tres valores posibles para esta categoría: ``MACHISTA'', ``NO\_MACHISTA'' y ``DUDOSO''.

Estos conjuntos de evaluación permiten intuir el rendimiento de los sistemas de clasificación y compararlo con el de otros sistemas. Asimismo, en los sistemas de clasificación supervisados, son clave para poder ser entrenados utilizando un subconjunto de la colección.

Para el presente trabajo, se utilizará como conjunto de evaluación del sistema de clasificación de contenido machista el corpus presentado en el capítulo 4. Se trata de un corpus compuesto por 3600 tweets recopilados mediante el uso de expresiones que pueden conllevar actitudes machistas. 

Para recuperar esta información se utilizaron los siguientes términos: ``feminazi", ``loca del", ``a la cocina", zorra, ``como una niña", ``las feministas", niñata, ``como una mujer", ``en tus días", ``a fregar", mojigata, marimacho, nenaza, ``para ser mujer", ``odio a las mujeres", lagartona, ``A las mujeres hay que", ``las mujeres no deberían", ``las mujeres de hoy en día", ``mujer al volante", ``mujer tenías que ser", ``mucho feminismo pero", ``pareces una puta", ``para ser chica". De este modo, se recopilaron todos los mensajes escritos en la red social que contuvieran estos términos durante las fechas 1/07/2018 - 31/12/2018.

El propósito principal de este corpus es la obtención de texto con alto contenido machista así como expresiones que, aún pudiendo ser machistas, no lo sean en ese contexto. De este modo, se pretende obtener un conjunto rico en el uso de expresiones que pueden conllevar actitudes machistas en diferentes contextos.

\subsection{Líneas base (\textit{baselines})}
\label{sec:Col_Eval}

\noindent Como se ha comentado en el capítulo 2, las referencias en el campo de detección del machismo son muy reducidas y, por tanto, es complejo encontrar algún trabajo comparable con el sistema desarrollado. Es por esto que en este trabajo se han desarrollado dos líneas base con las que comparar los resultados obtenidos por el sistema diseñado. La primera de ellas plantea un sistema de clasificación basado en una regresión logística sobre los atributos \textit{tf-idf} utilizando los unigramas de cada documento. De este modo, se plantea un sistema sencillo pero pudiendo ser, en ocasiones, mucho más efectivo que otras aproximaciones más complejas que utilizan bi-gramas o categorías gramaticales de los términos. Por tanto, se trata de un \textit{baseline} difícil de batir.

La segunda línea base está basada en un clasificador sencillo que predice siempre la clase mayoritaria. En este caso, como se puede observar en la tabla 4.7, la clase mayoritaria sería ``NO\_MACHISTA'' y, por tanto, este sistema clasificaría todos los registros de entrada con este valor de clase. La intención de esta línea base es comparar los resultados del sistema con otro hipotético no informado que no es capaz de ``aprender'' ningún patrón del conjunto de datos de entrenamiento.

\subsection{Experimento 1: Búsqueda de hiperparámetros mediante la optimización de la medida F1}
\label{sec:Col_Eval}

\noindent El primer experimento realizado trata de configurar cada algoritmo de clasificación para la tarea especifica que va a desarrollar. Como se introdujo en el capítulo 5, para la tarea de clasificación se emplean tres algoritmos distintos disponibles en ``scikit-learn'': Regresión logística, Random Forest y SVM. En este primer experimento, se realiza una búsqueda para los siguientes parámetros:

\begin{itemize}
	\itemsep0em 
	\item Regresión logística: C = [1, 10], class\_weight' = [None, 'balanced'].
	\item Random Forest: n\_estimators" = [250, 450], bootstrap' = (True, False), max\_depth'= [None, 30].
	\item SVM: C = [1, 10, 100, 10000], gamma = [0.001, 0.1, 0.6, 'auto'], kernel = 'rbf'.
\end{itemize}

Para ello, se sigue el procedimiento presentado en la figura 6.2 de forma iterativa. En el primer paso, se realizan 10 repartos o subconjuntos aleatorios del corpus en dos conjuntos de datos: entrenamiento (training) y testeo (testing). Para el conjunto de training, se reservan el 30\% de los datos y para el test, el resto. Para cada uno de los diez repartos, se utiliza el conjunto de training para la búsqueda de hiperparámetros y el testing para evaluar los resultados con los parámetros encontrados. Para la elección del reparto de los datos, se han realizado distintos experimentos. En primera instancia, se realizó un reparto 70-30\% de training y test para, posteriormente, ir realizando distintas pruebas hasta llegar al mejor resultado encontrado con un reparto de 30-70\%.

Para la búsqueda de parámetros, se realiza una validación cruzada (\textit{cross validation}) con cinco grupos realizando el entrenamiento en cuatro de ellos y el testeo en el grupo restante. Este proceso se repite para los cinco grupos y permite obtener los parámetros que mejor han funcionado en el proceso según el valor de la medida F1. 

En la siguiente etapa del proceso se utiliza el segundo conjunto de datos reservado para el testeo (el 70\% de todo el corpus) y se realiza la evaluación final. Utilizando los parámetros de entrada obtenidos en la etapa anterior, se realiza una validación cruzada con 10 grupos del conjunto de testing. De nuevo, en este proceso se realiza un reparto en 10 grupos, donde nueve de ellos serán utilizados para el entrenamiento y el grupo restante para el testeo, repitiendo el proceso diez veces, una por grupo. Estas dos etapas descritas se repiten para los diez repartos indicados al inicio.

Este experimento permite medir el resultado de un sistema diseñado especificamente para esta tarea pues la configuración de los algoritmos de clasificación se realiza según los datos del corpus. Además, al realizar diez iteraciones para el proceso, la varianza de los resultados se reduce y son menos dependientes del tipo de datos con el que se ha entrenado. La desventaja principal de este método es que es necesario reservar un conjunto de datos para la búsqueda de parámetros y se reduce la información de la que dispondrá el sistema de clasificación definitivo que realizará la predicción.

\begin{center}
	\includegraphics[scale=0.42,keepaspectratio]{imagenes/grid_search_cross_validation.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Búsqueda de hiperparámetros mediante la optimización de la medida F1}
\end{center}

\subsection{Experimento 2: Cross validation con parámetros por defecto}
\label{sec:Col_Eval}

\noindent El segundo experimento consiste en una única validación para todo el corpus utilizando los parámetros por defecto para todos los algoritmos de clasificación utilizados. En este caso, se ha optado por una validación cruzada con diez grupos. En la figura 6.3 se presenta un ejemplo equivalente para cinco grupos. 

En este procedimiento, se realiza una división del conjunto en 10 grupos del mismo tamaño del corpus y, de forma iterativa, se utilizarán nueve de ellos para el entrenamiento y el grupo restante para el testeo.

Este método permite evaluar un sistema más general cuyos parámetros de configuración no estén diseñados para los datos de entrenamiento de los que se disponen. De este modo, se podría mejorar la capacidad de generalización del sistema y evitar un posible sobreajuste.

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/cross_validation_5.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Validación cruzada k=5}
\end{center}

\section{Resultados experimento 1}
\label{sec:Metric_Eval}

La tabla 6.1 muestra los resultados promedio de exactitud (\textit{Accuracy}), medida F1, cobertura (\textit{Recall}) y precisión. Con el método de evaluación descrito para el primer experimento, el algoritmo \textit{Random Forest} alcanza una mayor tasa de acierto y precisión, mientras que la regresión logística alcanza los mejores resultados para la medida F1 y \textit{recall}. 

En relación a la comparación de los resultados obtenidos por el método con las dos líneas base, en este caso las diferencias con respecto a la aproximación basada en unigramas son de unos cuatro puntos porcentuales para cada medida. Por tanto, el sistema mejora esta primera aproximación en todas las medidas pero, como se preveía, la línea base ya es un buen punto de partida del sistema.

En el caso de la línea base basada en el clasificador sencillo de la clase mayoritaria, sí se pueden observar grandes diferencias en las métricas de calidad. Esto indica que cualquiera de las soluciones propuestas será mucho más adecuada que un clasificador basado en una única regla sencilla.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}lllll@{}}
		\toprule
		& \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} \\ \midrule
		\textbf{Baseline (tf-idf)} & 0.68 & 0.59 & 0.62 & 0.59 \\
		\textbf{Baseline} & 0.61 & 0.2 & 0.3 & 0.24 \\
		\textbf{LR} & 0.7 & \textbf{0.62} & \textbf{0.64} & 0.62 \\
		\textbf{RF} & \textbf{0.72} & 0.6 & 0.57 & \textbf{0.67} \\
		\textbf{SVM} & 0.7 & 0.61 & 0.63 & 0.61 \\ \bottomrule
	\end{tabular}
	\caption{Resultados experimento 1}
	\label{tab:my-table}
\end{table}

Los resultados presentados en la tabla 6.1 muestran que el sistema de clasificación desarrollado en el presente trabajo se comporta mejor que las lineas base en todas las métricas de calidad. Estos buenos resultados parecen confirmar la hipótesis de que cualquier modelo utilizando todos los atributos disponibles en el corpus resulta más apropiado que solo el uso del texto disponible para cada registro.

En concreto, la diferencia entre el mejor de los clasificadores del método propuesto y la línea base basada en unigramas es de 4\%. Esta diferencia se debe principalmente a la información que aportan los atributos numéricos y categóricos para la tarea del clasificación. Esto se confirma en la figura 4.9, donde se puede observar como la longitud del tweet y el número de \textit{retweets} tienen influencia en la clasificación. Se puede intuir como en el caso de los tweets dudosos, la longitud media es mucho menor que en el caso de los machistas y no machistas. Además, los tweets dudosos no presentan gran cantidad de \textit{retweets} si se comparan con el resto de clases. Es por esto, que utilizar estos atributos en el sistema de clasificación aporta información útil y permite mejorar los resultados.

Esta hipótesis se reafirma con los resultados de la figura 6.4. En esta figura, se observan los valores SHAP (\textit{SHapley Additive exPlanations}) \cite{shap} para los tweets dudosos. Esta técnicas permite desglosar cada predicción individualmente para mostrar el impacto que tiene cada atributo en la clasificación. El primer atributo (\textit{display\_text\_width}) coincide con la longitud del tweet, y se confirma que, a menor valor en este atributo (menor longitud del tweet), más probable es que el sistema clasifique el texto como dudoso. Cabe destacar otros atributos como los términos ``nenaza'' y ``feminazi'' en los que se observa que la presencia de estos términos reduce la probabilidad de que el sistema clasifique el tweet como dudoso. Esto se debe a que la existencia de estos términos en los tweets se han asociado con comportamientos machistas durante el proceso de etiquetado, como se puede puede observar en las figuras 6.5 y 6.6. Otro atributo interesante que provocó inconvenientes durante el proceso de etiquetado manual del corpus es la existencia del término ``lagartona''. En este caso, la existencia de este término provoca que aumente la probabilidad de que el tweet sea dudoso, esto ocurre por la ambigüedad de la palabra que puede hacer referencia a una persona que vende su cuerpo a cambio de dinero o a una persona pícara. Esta confusión provoca que, si existe el término en un mensaje, aumente la probabilidad de ser dudoso. Otro término que ha provocado confusión durante el etiquetado ha sido ``niñata''. Como se puede observar en la figura 6.6, la existencia de este término aumenta la probabilidad de que el tweet sea no machista pues, por sí solo, este término no conlleva una actitud machista. A pesar de esto, se trata de un término que, según el contexto en el que sea empleado, podría ser considerado machista.

Estos atributos numéricos y categóricos influyen también para el resto de valores de la clase. De hecho, se puede observar en la figura 6.7 cómo las tres primeras variables en importancia son variables no textuales. Por ejemplo, la variable ``statuses\_count'' indica la cantidad de mensajes emitidos por el usuario que publica el tweet y, en este sistema, tiene un impacto razonable para clasificar los tweets como machistas y no machistas. Esto podría explicarse por el hecho de que, cuanto más activo sea el usuario en la red social, más probable es que exprese sus ideas acerca del machismo, ya sea a favor o en contra, con el uso de expresiones machistas.

%statuses\_count :La cantidad de Tweets (incluidos los retweets) emitidos por el usuario. 
%listed\_count :Número de listas públicas de las que el usuario es miembro.

\begin{center}
	\includegraphics[scale=0.42,keepaspectratio]{imagenes/shap_values_dudosos.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para tweets dudosos}
\end{center}

\begin{center}
	\includegraphics[scale=0.42,keepaspectratio]{imagenes/shap_values_machistas.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para tweets machistas}
\end{center}

\begin{center}
	\includegraphics[scale=0.42,keepaspectratio]{imagenes/shap_values_no_machistas.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para tweets no machistas}
\end{center}

\begin{center}
	\includegraphics[scale=0.42,keepaspectratio]{imagenes/shap_values_impacto.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Impacto de los atributos}
\end{center}

La tabla 6.1 muestra también ligeras diferencias entre los resultados obtenidos según el algoritmo de clasificación utilizado. En este experimento, RF y LR obtienen el mejor valor para dos de las cuatro métricas de calidad evaluadas, sin embargo, las diferencias con SVM no son demasiado amplias. 

A priori, cabía esperar mejores resultados para el clasificador SVM pues ha sido aplicado de un modo exitoso en referencias previas para clasificación textual \cite{Canos2018,Wahyu2018}. Este tipo de algoritmo suele funcionar mejor que otros como los árboles de decisión en problemas donde las matrices de datos son muy ``dispersas'' y el concepto de distancia entre los diferentes puntos adquiere mayor importancia, como ocurre al realizar la representación de texto mediante los atributos tf-idf. Sin embargo, en este problema en particular, solo se trabajan con 222 atributos en total antes de aplicar el algoritmo de clasificación, esto podría provocar que este tipo de técnica deje de tener la efectividad esperada.

En cuanto a los resultados obtenidos con el algoritmo RF, es importante señalar que se han conseguido unos valores elevados en la tasa de acierto y en precisión debido a la gran capacidad de la técnica para detectar los tweets no machistas. Esto se puede confirmar mediante la matriz de confusión representada en la tabla 6.2, donde se observa que el valor de clase ``NO\_MACHISTA'' es claramente el que mejor se clasifica mediante esta técnica. 

La elevada precisión del algoritmo RF para los tweets no machistas se debe al efecto del desbalanceo de la clase. Como ya se introdujo en los capítulos anteriores, y debido a la naturaleza del problema, existe una gran predominancia de los tweets no machistas en el corpus etiquetado manualmente. Esto provoca que algoritmos como los árboles de decisión, puedan tener un sesgo en sus predicciones hacia la clase predominante \cite{Liu}. Otro tipo de técnicas como SVM pueden alcanzar mejores rendimientos en este tipo de problemas \cite{Ustuner2016}. Pese a esto, RF no se aleja demasiado en otras métricas como F1 por el uso conjunto de numerosos árboles de decisión, que permite mejorar el rendimiento que se obtendría con un único árbol de decisión.

%%
% Se puede mejorar la explicación: mostrar la estructura de los árboles y obtener importancia de atributos

En cuanto a los algoritmos SVM y LR, los resultados muestran un comportamiento muy similar en ambos casos. Esto se debe principalmente al uso de un ``kernel'' lineal para el algoritmo SVM, lo que provoca que la frontera de decisión entre clases sea lineal, al igual que ocurre con el algoritmo LR. En ambos casos, los resultados muestran una menor capacidad para detectar los tweets no machistas pero se mejora el equilibrio entre el resto de valores de la clase. En la tabla 6.3, se puede observar cómo estos modelos mejoran notablemente la detección de tweets dudosos y machistas, lo que indica un mejor funcionamiento de estos métodos ante clases desbalanceadas.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}llll@{}}
		\toprule
		\textbf{RF} & \textbf{DUDOSO} & \textbf{MACHISTA} & \textbf{NO\_MACHISTA} \\ \midrule
		\textbf{DUDOSO} & 70 & 32 & 80 \\
		\textbf{MACHISTA} & 31 & 407 & 387 \\
		\textbf{NO\_MACHISTA} & 39 & 124 & 1350 \\ \bottomrule
	\end{tabular}
	\caption{Matriz de confusión para una iteración de RF}
	\label{tab:my-table}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}llll@{}}
		\toprule
		\textbf{LR} & \textbf{DUDOSO} & \textbf{MACHISTA} & \textbf{NO\_MACHISTA} \\ \midrule
		\textbf{DUDOSO} & 116 & 36 & 36 \\
		\textbf{MACHISTA} & 96 & 479 & 247 \\
		\textbf{NO\_MACHISTA} & 91 & 250 & 1169 \\ \bottomrule
	\end{tabular}
	\caption{Matriz de confusión para una iteración de LR}
	\label{tab:my-table}
\end{table}

Es importante también destacar que, en cualquiera de los dos modelos de clasificación, la principal fuente de errores proviene de la detección de tweets machistas. En la tabla 6.3, se puede observar cómo en esta clase se produce la mayor parte de los errores, clasificándose erróneamente más del el 30\% de los tweets machistas. En concreto, la mayor parte de los errores se produce en la clasificación de tweets de este tipo como tweets no machistas. Para estudiar esta fuente de error, se ha realizado un estudio en profundidad de las clasificaciones producidas. Por ejemplo, el tweet ``\textit{@CopitoDeSnow\_ Ahora es cuando digo ``no está mal para ser mujer''}'' se ha clasificado erroneamente como no machista por el sistema. En la figura 6.8 se representa la contribución de los atributos más importantes para la clasificación ofrecida por el sistema. En este caso, la existencia de los términos ``ser'' y ``digo'' además de la inexistencia del término ``nenaza'', aumentan la probabilidad de que el sistema clasifique el tweet como no machista, mientras que los atributos que disminuyen la probabilidad de esta clasificación serían el tamaño del tweet y la publicación de éste desde un dispositivo Android. Como se puede observar, no se detectan términos textuales que reduzcan la clasificación de no machista y, por ello, el tweet se clasifica erróneamente. Para este ejemplo, el sistema falla en la detección del grupo ``mal para ser mujer'' como una expresión que claramente conlleva una actitud machista.

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej1.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 1}
\end{center}

Otro ejemplo de error de este tipo sería el tweet ``\textit{La  negativa a q el aborto sea legal, lleva implícito el elemento castigo: si follaste, pare.Porque las mujeres, no deberían follar tan alegremente, ni que fueran  hombres}''. De nuevo, se puede observar en la figura 6.9 como la inexistencia del término ``nenaza'' aumenta la probabilidad de que el tweet sea no machista. Esto se produce porque, en general, todas las instancias que contenían este término se han considerado machistas y, por tanto, el modelo está sesgado hacia los valores de clase que se le presentaron durante el entrenamiento. Es interesante remarcar, que el término ``deberían'' contribuye a reducir la probabilidad de que el tweet se considere no machista, sin embargo, no lo suficiente para evitar el error del clasificador. Este término se repite mucho en el corpus para dar opiniones machistas acerca de cómo deben de comportarse las mujeres o colectivos feministas.


\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej2.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 2}
\end{center}

De nuevo, el sistema produce un error en la clasificación del tweet ``\textit{@damita2808 @berege7 @Mariagtriana Y los ojos? Uff demasiado dureza en la mirada para ser chica...no?}''. Al igual que ocurre en los ejemplos anteriores, la inexistencia del término ``nenaza'' contribuye a clasificar el tweet como no machista, al mismo tiempo que el sistema falla al detectar algunos de los términos de la expresión ``demasiado para ser chica''. Esto se produce, al igual que en los casos anteriores, por la limitación de la aproximación basada en unigramas que utiliza frecuencias de términos para representar el texto a clasificar. Esto supone una desventaja en corpus como el que se estudia en este trabajo, donde el vocabulario es muy heterogéneo y las expresiones o grupos de palabras clave no se repiten lo suficiente durante la etapa de entrenamiento del sistema. En este corpus, se trabaja con 222 atributos, lo que indica que se opera con un vocabulario limitado a menos de 200 términos que se repiten en al menos el 1\% de los tweets. Se han realizado pruebas reduciendo este umbral hasta un 0.3\% para aumentar el número de términos del vocabulario pero los resultados no han mejorado. Este mismo efecto se produce en el tweet  ``\textit{Las mujeres no deberían usar sostén}'' (figura 6.11) donde los términos textuales no tienen un peso relevante al realizar la clasificación. Por tanto, por un lado, hay que tener en cuenta que, pese a realizar el corpus mediante la búsqueda de unos términos bien definidos, la ambigüedad y riqueza del lenguaje provoca que el contexto en el que se usan sea muy diverso, más aún al tratarse de una red social. Por otro lado, el método aplicado puede tener limitaciones importantes para trabajar en este contexto pues está basado en frecuencias de términos y empeora en entornos donde el uso de éstos no es homogéneo.

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej3.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 3}
\end{center}

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej7.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 4}
\end{center}



Otro efecto relevante se produce en los siguientes tweets clasificados erróneamente como no machistas por el sistema:

	\textit{@EsppeonzAguirre "Por ejemplo,@IrantzuVarela hace unos sketches poniéndose unas barbas postizas despeinadas. Es lo que antes se llamaba un marimacho. El igualitarismo ha hecho mucho daño. Uno tiene que mandar y otro obedecer acríticamente}
	
	\textit{Buscad mujeres con valores. No prestéis atención a ninguna niñata feminista. No os relacionéis con ellas, salvo para educarlas. No dejemos que nos coma el NOM.}

En ambos casos, se comparten tweets de una longitud considerable donde no existe ningún término individual que indique una actitud machista sino un conjunto de ellos que denotan este tipo de comportamientos. Por esto, se puede observar en las figuras 6.12 y 6.13 cómo los términos individuales más importantes en el corpus como ``marimacho'', ``niñata'' o ``feminista'' presentes en los tweets ``compiten'' para realizar la clasificación. La limitación de la solución propuesta para tener en cuenta el contexto provoca el error en estos casos. Para intentar reducir este efecto y considerar grupos de palabras, se han realizado pruebas incluyendo atributos tf-idf basados en bigramas aunque sin éxito en los resultados.

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej5.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 5}
\end{center}

\begin{center}
	\includegraphics[scale=0.55,keepaspectratio]{imagenes/shap_values_ej4.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Valores SHAP para ejemplo 6}
\end{center}

Otra fuente importante de errores que se ha detectado son las faltas de ortografía cometidas por los usuarios al utilizar la red social. Pese a que se ha utilizado un léxico para realizar un mapeo de las faltas más comunes, estos errores hacen que el sistema no pueda representar correctamente los términos como atributos del conjunto de datos.

\section{Resultados experimento 2}
\label{sec:Col_Eval}

La tabla 6.2 muestra los resultados promedio de exactitud (\textit{Accuracy}), medida F1, cobertura (\textit{Recall}) y precisión. Para este segundo experimento, el algoritmo RF consigue de nuevo la mejor tasa de acierto y precisión, mientras que, en este caso, es el algoritmo SVM el que obtiene los mejores resultados en cuanto a la medida F1 y el \textit{recall}. Sin embargo, es importante destacar que el comportamiento de los tres clasificadores es bastante similar en líneas generales.

En relación con el método de evaluación propuesto en el experimento 1, se puede observar una pequeña mejoraría de un punto para la tasa de acierto, la precisión y la medida F1. Sin embargo, se mantienen muy similares los comportamientos de cada algoritmo de predicción. Este efecto se puede deber a que los parámetros que obtenemos del método 2 no son tan buenos por los pocos datos que utilizamos (solo 25\% del corpus), y modifican muy poco el comportamiento de éstos por defecto.

Se ha observado, mediante distintos repartos de datos en las etapas de búsqueda de parámetros y evaluación, que aumentar la cantidad de información disponible para la validación cruzada, mejoraba siempre los resultados independientemente del valor de los parámetros de entrada de los algoritmos. Es decir, añadir más información a la etapa de búsqueda de parámetros no ha resultado tan efectivo como reservar la mayor parte de la información para entrenar y evaluar el sistema final.

En relación a la comparación de los resultados obtenidos por el método con las dos líneas bases, de nuevo, se mantienen las mismas tasa de mejoras que en el experimento anterior.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}lllll@{}}
		\toprule
		& \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} \\ \midrule
		\textbf{Baseline (tf-idf)} & 0.69 & 0.58 & 0.56 & 0.62 \\
		\textbf{Baseline} & 0.61 & 0.2 & 0.3 & 0.24 \\
		\textbf{LR} & 0.72 & \textbf{0.63} & 0.62 & 0.65 \\
		\textbf{RF} & \textbf{0.73} & 0.61 & 0.58 & \textbf{0.68} \\
		\textbf{SVM} & 0.72 & \textbf{0.63} & \textbf{0.64} & 0.63 \\ \bottomrule
	\end{tabular}
	\caption{Resultados experimento 2}
	\label{tab:my-table}
\end{table}

En cuanto a las diferencias entre los modelos empleados, se observan las mismas relaciones y los efectos producidos son equivalentes al experimento anterior. Si se observan detenidamente los resultados, se observará cómo las medidas de calidad que más aumentan respecto del experimento 1 serían la tasa de acierto y la precisión (la medida F1 aumenta como consecuencia de la precisión). Esto se produce porque, en realidad, este experimento detecta algunos casos más de tweets no machistas, probablemente porque se presenta más cantidad de información en el entrenamiento. Por tanto, la detección de tweets machistas seguiría siendo la principal fuente de error del sistema. Se ha comprobado, mediante un estudio de los errores, que se dan los mismos efectos descritos en el experimento 1.


\section{Efecto del desbalanceo de la clase}
\label{sec:Resultados}
\noindent Los resultados presentados hasta este punto, demuestran que el problema principal del sistema desarrollado es el sesgo producido hacia el valor de la clase mayoritaria.

Para evaluar el impacto del desbalanceo de la clase hacia tweets no machistas e intentar paliar este efecto, se ha realizado un experimento mediante un muestreo del corpus. Como se puede observar en la tabla 4.7, el valor de la clase ``DUDOSO'' sería el que cuenta con menos instancias dentro del corpus. Por esto, se ha realizado una prueba mediante un muestreo aleatorio de las otras dos clases para igualar la cantidad de registros entre las tres clases que componen el corpus. 

En la tabla 6.5 se pueden observar los resultados para cada uno de los métodos empleados en el presente trabajo, evaluados mediante una validación cruzada con 10 grupos y los parámetros por defecto (mismas condiciones que el experimento 2).

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}lllll@{}}
		\toprule
		& \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} \\ \midrule
		\textbf{LR} & 0.61 & 0.61 & 0.61 & 0.62 \\
		\textbf{RF} & 0.68 & 0.68 & 0.68 & 0.68 \\
		\textbf{SVM} & 0.63 & 0.63 & 0.66 & 0.63 \\ \bottomrule
	\end{tabular}
	\caption{Resultados experimento 2 con balanceo de clases}
	\label{tab:my-table}
\end{table}

Como se puede observar, los resultados empeoran generalmente para los parámetros de calidad evaluados pese a existir un equilibrio total en el número de registros por clase. Los algoritmos LR y SVM son los que más empeoran en este caso, una posible causa podría ser la escasa información con la que cuenta el sistema, en este caso, 267 instancias por clase.

El único algoritmo que consigue mejorar alguna de sus medidas de calidad es RF. En este caso, se mejora notablemente la medida F1 y la cobertura, lo que indica que, en condiciones de equilibrio entre las clases, este algoritmo mejora notablemente la detección del resto de valores de la clase del corpus.





