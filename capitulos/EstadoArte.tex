%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ESTADO DEL ARTE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Estado del arte}
\fancyhead[RE]{\textsc{CAP\'ITULO} \thechapter. Estado del arte}
\label{ch:EstadoArte}

\noindent El presente capítulo tiene como objetivo presentar al lector la detección del lenguaje machista en redes sociales. Para ello, se realizará una revisión de los trabajos más relevantes en la tarea de detección de lenguaje abusivo y machista, en los que se analizarán los orígenes de esta tarea, las soluciones técnicas y las aportaciones más relevantes.


\section{Clasificación de textos}
\label{sec:Ejemplo_seccion}

\noindent El procesamiento del lenguaje natural (\textit{``Natural Language Processing''}, NLP) tiene como objetivo fundamental el desarrollo de métodos que permitan a los computadores realizar tareas relacionadas con el lenguaje humano, como la comunicación o el procesamiento de textos. La principal diferencia del NLP con el resto de líneas de investigación relacionadas con el análisis de datos o la inteligencia artificial es la necesidad de un conocimiento del lenguaje en todas sus aplicaciones. Elementos clave del lenguaje como la fonética, la fonología, la morfología, la sintaxis, la semántica, la pragmática y la discursiva son esenciales en cualquier técnica de procesamiento del lenguaje \cite[Capítulo~1]{Jurafsky}.

Una de las áreas más importantes de investigación relacionadas con el NLP es la clasificación de textos o documentos. De un modo general, se conoce como clasificación automática \cite[Capítulo~4]{Jurafsky} a la tarea de asignar una o varias categorías predefinidas sobre una colección de instancias a clasificar. Del mismo modo, la clasificación de textos \cite[Capítulo~4]{Jurafsky} se puede entender como aquella tarea en la que un documento o texto es etiquetado como perteneciente a un determinado conjunto. Este tipo de técnicas se utilizan para un gran número de aplicaciones, tales como:
\begin{itemize}
	\itemsep0em 
	\item Indexación para sistemas de recuperación de información \cite[Chapter~17]{Jurafsky}
	\item Detección de \textit{spam} \cite{Hai2010,Jurafsky}
	\item Identificación del lenguaje \cite[Chapter~3]{Jurafsky}
	\item Análisis de sentimientos \cite[Chapter~4]{Jurafsky}
	\item Organización de documentos \cite{Mauro2016}
	\item Desambigüación del sentido de las palabras \cite{Russell2002}
	\item Filtrado de textos \cite{Mark}
\end{itemize}

Formalmente, el problema se define como un texto o documento $d$ que puede pertenecer a un conjunto fijo de clases $C=\{c_1,c_2,...,c_i\}$. La salida del sistema es la predicción la clase $c \in C$.

Para resolver el problema de la clasificación de textos existen dos enfoques principales: uno basado en reglas y otro mediante algoritmos de clasificación supervisado.

Los sistemas basados en reglas utilizan patrones predefinidos por un experto para crear un conjunto de pautas mediante la combinación de palabras u otros atributos \cite{Elizabeth2001}. En este tipo de arquitecturas, la precisión puede ser alta siempre que estas reglas estén cuidadosamente seleccionadas por un experto. Sin embargo, dichos sistemas resultan muy costosos de construir y mantener. Además, se trata de sistemas muy específicos para un dominio o problema concreto, y difícilmente trasladables a un dominio distinto.

El aprendizaje supervisado se construye sobre un conocimiento a priori. Se debe disponer de un conjunto de documentos de ejemplo para cada una de las categorías consideradas. Después de una etapa de entrenamiento, el sistema queda ajustado de modo que, ante nuevos ejemplos, el algoritmo es capaz de clasificarlos en alguna de las clases existentes. Para este tipo de sistemas se utilizan distintos modelos de clasificadores: \textit{Naive Bayes}, \textit{Regresión logística}, \textit{SVM}, \textit{redes neuronales}, etc \cite{Aurangzeb2010}.

Para construir cualquier clasificador de textos o documentos es necesario seguir los siguientes pasos:

\begin{itemize}
	\itemsep0em 
	\item Extraer los atributos o \textit{features} necesarias para realizar una representación fiel del texto y que permita la utilización de un algoritmo de clasificación
	\item Desarrollar procedimientos por los cuales los documentos puedan ser clasificados automáticamente dentro de
	categorías.
	\item Evaluar la calidad de la clasificación en relación a algún criterio.
\end{itemize}


\subsection{Representación textual}
\label{sec:Ejemplo_subSeccion} 


\subsubsection{Modelo de espacio vectorial}
\label{sec:Ejemplo_subSeccion} 

\noindent La representación del texto es un paso fundamental para el procesamiento automático de textos. Una representación fiel al contenido del documento, que incluya la información necesaria para extraer conocimiento útil, será clave para el desarrollo de una arquitectura con un rendimiento adecuado. En este proceso, se han de tener en cuenta las especificaciones de los algoritmos que se empleen a continuación.

En esta fase, se definen todos los atributos utilizados en el paso posterior por el algoritmo de clasificación. Los atributos seleccionados o generados a partir de los originales serán los que marquen el éxito de la arquitectura completa. La elección del algoritmo de clasificación para los pasos posteriores influirá de un modo mucho menos significativo. Por ejemplo, en \cite{Victor2018} y \cite{Wahyu2018} se utiliza el mismo algoritmo de clasificación pero los resultados son muy diferentes debido a los atributos utilizados. 

Un modelo de representación muy utilizado se conoce como modelo de representación vectorial \cite{Fresno2006}. Mediante esta representación, los documentos se modelan como vectores dentro de un espacio euclídeo. De este modo, se pueden aplicar operaciones de distancia entre vectores, como indicador de su cercanía según el contenido textual. En la siguiente imagen se muestra un ejemplo en dos dimensiones:

\begin{center}
	\includegraphics[width=0.4\textwidth]{imagenes/modelo_vectorial.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Representación vector de documentos}	
\end{center}

En este caso, se tendría un vocabulario con únicamente dos rasgos $w_1$ y $w_2$ que conforman el espacio en el que se encuentran los documentos o textos $d_1$ y $d_2$. De este modo, se pueden emplear medidas de distancia, como la distancia euclídea o la distancia coseno, para comparar ambos documentos.

Utilizando este modelo, un texto quedará representado como una combinación lineal de vectores, donde cada coeficiente representa la relevancia de cada rasgo en el contenido del texto, calculado con una función de pesado. Para un texto $d$, un vocabulario de tamaño $n$: $\vec{d}=t_1j \vec{t_1} + ... + t_nj \vec{t_n} $. Para el cálculo de la relevancia de cada rasgo $t_nj$, se utilizará una función de pesado. Una de las más utilizadas se conoce como TF-IDF (frecuencia del termino x frecuencia inversa del documento) y se calcularía del siguiente modo: \[TF-IDF(\vec{t_i},\vec{d_j})=f_{ij}\log(\frac{N}{d_f(\vec{t_i})})\] donde $N$ es la dimensión del corpus (en este caso número de tweets), $f_{ij}$ la frecuencia del término en el documento y $d_f(\vec{t_i})$ el número de documentos (en este caso el tweet) en los que aparece el término.


\subsubsection{Representación semántica mediante modelo de espacio vectorial}
\label{sec:Ejemplo_subSeccion} 

\noindent En el apartado anterior, se ha descrito un procedimiento para la representación textual que permite expresar la información contenida en el texto como una combinación lineal de vectores, donde cada coeficiente representa la existencia o no de los términos contenidos en el vocabulario del corpus. Sin embargo, una desventaja de este método es su incapacidad para representar la información semántica de los documentos. Es decir, este método no es capaz de capturar información acerca del significado o concepto que contienen los términos presentes en los documentos. Por ejemplo, considérense los siguientes documentos:

	$d_1 = $``\textit{Pizza margarita}''

	$d_2 = $``\textit{Pizza calzone}''

	$d_3 = $``\textit{Macarrones}''

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\textbf{Términos} & \textbf{Pizza} & \textbf{margarita} & \textbf{calzone} & \textbf{Macarrones} \\ \midrule
		\textbf{$d_1$} & 1 & 1 & 0 & 0 \\
		\textbf{$d_2$} & 1 & 0 & 1 & 0 \\
		\textbf{$d_3$} & 0 & 0 & 0 & 1 \\ \bottomrule
	\end{tabular}
	\caption{Matriz documento - término}
	\label{tab:my-table}
\end{table}

Si se realiza una representación de los documentos según el modelo descrito anteriormente (con el número de términos como función de pesado) se obtendría la matriz de la tabla 2.1. Como se puede observar, los documentos $d_1$ y $d_2$ tienen en común un atributo mientras que el documento $d_3$ no tendría ningún rasgo común con el resto de documentos. Sin embargo, si se tiene en cuenta el significado o el concepto que expresan los documentos, se concluiría que todos los documentos representan comida italiana y, por tanto, todos tienen en común ese rasgo semántico. Esto indica que si un documento se trata únicamente como una sucesión de términos, se perderán los conceptos a los que hace referencia.

Los métodos para la generación automática de atributos semánticos fueron desarrollados alrededor del año 1990 con el objetivo de capturar la información semántica o los conceptos presentes en los documentos. Uno de los algoritmos más populares dentro de esta familia se conoce como LSA (\textit{Latent Semantic Analysis}) \cite{Deerwester1990} que describe a cada palabra o conjunto de palabras en un espacio vectorial de dimensión reducida en el que las palabras semánticamente similares se encontrarán más próximas. Para ello, este método toma como entrada una matriz término-documento a la que se le aplica una descomposición de valores singulares (\textit{Singular Value Descomposition}, SVD) obteniendo así una representación vectorial tanto de los términos como de los documentos. De este modo, se puede cuantificar la similitud semántica de dos palabras mediante el uso de medidas de distancias entre vectores. Esta técnica fue rápidamente aplicada a distintos dominios como la representación de un modelo cognitivo \cite{Landauer1997}, modelo de lenguaje \cite{Jurafsky1998} o calificador automático \cite{Rehder1998}.

Aunque el método LSA se planteó como una variante al modelo de espacio vectorial, hace uso de las mismas formas de representación a la entrada del método. Aquí, el texto se representa en un espacio de coordenadas donde los documentos y los términos se expresan como una combinación de factores semánticos o \textit{topics} subyacentes. De este modo, cuando las palabras están relacionadas, por ejemplo, ``\textit{Pizza}'', ``\textit{Lasaña}'' y ``\textit{Macarrones}'', cabrá esperar que aparecieran en conjuntos similares de documentos. 

Formalmente, el LSA es la aplicación de la función SVD a la matriz término documento. Esta matriz $A_{[mxn]}$ almacena en sus componentes $a_{ij}$ información relativa a las frecuencias de los términos $t_i$ en los documentos $d_j$. Un ejemplo de esta matriz se obtendría aplicando la transpuesta $A^T$ a la matriz traspuesta de la tabla 2.1. La proyección SVD se calcula descomponiendo la matriz $A_{[mxn]}$ en un producto de tres matrices:


\[A_{[mxn]}=U_{[mxr]}\Sigma_{[rxr]}(V_{[nxr]})^T\] donde $r$ toma valores entre 1 y $\min(\dim(\vec{t_i}, d))$. Este valor $r$ se puede considerar como el número de rasgos semánticos o \textit{topics} (temas) en los que se agruparán los documentos. En esta expresión, las matrices $V$ y $U$ están formadas por vectores ortonormales (el módulo de cada vector columna es 1 y todos ellos son linealmente independientes). De este modo, $V^T V = U^T U = I$ mientras que $\Sigma$ representa una matriz diagonal en la que sus elementos están ordenados en orden decreciente.

En resumen, SVD encuentra la proyección óptima a un espacio de dimensión reducida explotando patrones de coaparición entre términos. En este proceso, aquellos rasgos que tienen patrones similares son proyectados a una misma dirección. Estos patrones tratan de inferir similitud semántica entre términos, lo que en ocasiones no es acertado \cite{Manning1990}. Sin embargo, en muchos casos ha demostrado ser un indicador válido de relaciones temáticas. En \cite{Foltz1996}, la representación del documento se reduce a un conjunto de 20 palabras y se consideran conjuntos de sintagmas, tratados como atributos, para crear el vector de características, usando la similitud normalizada entre fragmentos de textos.


\subsubsection{\textit{Word Embedding}}
\label{sec:Ejemplo_subSeccion}

\noindent En las secciones previas se han presentado dos métodos que hacen uso del modelo de espacio vectorial para representar los documentos. En el primer apartado, se explicó cómo representar texto mediante un vector en el que las dimensiones corresponden a las palabras en el vocabulario. En esta sección, se presenta un método alternativo para representar términos mediante el uso de vectores de números reales de menor tamaño y más densos (más valores distintos de cero). Este tipo de técnicas se conocen como \textit{word embedding} y han tenido un éxito enorme en numerosas tareas relacionadas con el procesamiento natural del lenguaje en los últimos años \cite{Jacob2019} superando a los métodos empleados tradicionalmente como la técnica LSA \cite{Baroni2014} presentada en el apartado anterior. Mientras que los métodos explicados hasta ahora utilizan el conteo de términos para representar los documentos, los métodos basados en \textit{word embedding} pueden ser vistos como modelos de predicción que intentan predecir los términos alrededor de las palabras (o a la inversa). De este modo, estos métodos son capaces de tener en cuenta el contexto en el que se utilizan los términos presentes en el corpus, consiguiendo así capturar la información semántica en su representación.

Como se ha comentado, los vectores obtenidos mediante \textit{word embedding} son más densos y funcionan mejor que las representaciones tradicionales. Aunque no se entienden todas las razones de esta mejora, se tienen ciertas ideas \cite[Capítulo~6]{Jurafsky}. Primero, estos vectores pueden utilizarse de un modo más adecuado como atributos en los sistemas de aprendizaje de máquina debido a la gran reducción de los vectores con los que se trabaja. Asimismo, esta reducción de coeficiente puede ayudar a mejorar la capacidad de generalización del clasificador y prevenir el sobreajuste. Finalmente, las técnicas de \textit{word embedding} son capaces de capturar la información semántica de los términos. Por ejemplo, los documentos de la tabla 2.1 serán próximos entre sí si se utilizaran este tipo de métodos para representar los términos que contienen.

El término \textit{word embeddings} fue originalmente acuñado en \cite{Bengio2003}, sin embargo, fue en  \cite{Collobert2008} donde se demostró todas las ventajas de este tipo de metodología. Otro hito importante dentro de esta línea de trabajo fue la publicación de \cite{Mikolov2013} en la que se presenta la librería \textit{word2vec}, una de las herramienta más populares que permite entrenar y utilizar modelos pre-entrenados de \textit{word embedding}. Un año después, fue presentado \textit{GloVe} \cite{Pennington2014} confirmando la popularidad de las técnicas de \textit{word embedding}.

El algoritmo \textit{word2vec} se basa en dos técnicas principales: \textit{skipgram} \cite{Mikolovb2013} y \textit{negative sampling} \cite{Gutmann2012}. El método \textit{skipgram} es un modelo del lenguaje que, tomando un término como referencia, intenta predecir una ventana de términos alrededor de él. Para ilustrar el proceso, considérese el ejemplo ``Un coche atropelló a David'' y una ventana de cinco palabras (dos hacia delante y dos hacia atrás del término central). El primer paso del algoritmo es generar el conjunto de datos con el que entrenar el modelo de predicción, para ello, se realiza el proceso de la figura 2.2. En la primera iteración, se desplaza la ventana hasta el primer término (como atributo de entrada) y se toman los dos términos posteriores (como salida del modelo de predicción), en este caso, se generan los dos ejemplos de entrenamiento \textit{(Un, coche)} y \textit{(Un, atropelló)}. Este proceso se repite de modo iterativo para todos los términos del corpus, generando un corpus de entrenamiento como el de la figura 2.2.

\begin{center}
	\includegraphics[scale=0.4,keepaspectratio]{imagenes/ex_skipgram.png} %[width=4cm,,keepaspectratio]
	\captionof{figure}{Ejemplo algoritmo skpigram}	
\end{center}

La principal desventaja de utilizar el corpus generado en la figura 2.2 es que, al buscar una palabra del vocabulario como salida del modelo, es necesario indicar una probabilidad para cada elemento del vocabulario. Esto puede ser computacionalmente muy costoso más aún si tenemos en cuenta que pueden existir millones de términos en un único vocabulario. Por ello, en lugar de intentar predecir la palabra ``vecina'' dado un término de entrada, se modifica el enfoque y se realiza la predicción de la probabilidad de que dos palabras sean ``vecinas'' dados dos términos de entrada. De este modo, el corpus de la figura 2.2 se verá modificado y se tendrán ambas palabras como entrada al modelo de predicción y, como etiqueta, se utiliza un ``1'' habitualmente para representar los términos vecinos y un ``0'' para el resto de casos. Sin embargo, en la figura 2.2 todos los ejemplos son vecinos y, por tanto, el corpus solo tendría ejemplos de este tipo para el entrenamiento. Para un correcto funcionamiento del clasificador, es necesario presentar patrones de entrada que no sean vecinos y, para ello, se utiliza el procedimiento de \textit{negative sampling} o muestreo negativo. En este proceso, se muestrean varios ejemplos negativos (términos que no sean vecinos) para cada caso utilizando el corpus existente. Por tanto, el corpus de la figura 2.2 se completaría con ejemplos negativos de términos de otros documentos del corpus, por ejemplo, en la primera iteración se podría añadir los patrones \textit{(Un, saltar, 0)} o \textit{(Un, violeta, 0)} como ejemplos negativos. Esta técnica aplicada junto con el método \textit{skipgram}, constituyen los dos elementos más importantes en el proceso de entrenamiento de \textit{word2vec}.

Previo al entrenamiento del algoritmo, se establece un tamaño de vocabulario, por ejemplo, 10.000 términos. Tras esto, se crean dos matrices: la matriz \textit{embedding} y la matriz de contexto. Estas dos matrices tendrán tamaño $[vxe]$ done $v$ corresponde al tamaño del vocabulario y $e$ al número de elementos que contendrá el vector final de números reales con el que se codificará el texto. Estas dos matrices almacenan las palabras del vocabulario y se inicializan sus vectores asociados con valores aleatorios para comenzar el proceso de entrenamiento.

Para comenzar el entrenamiento, se toma un ejemplo positivo y sus ejemplos negativos asociados. Para el primer caso de la figura 2.2, se tomaría el ejemplo positivo \textit{(Un, coche, 1)} y sus ejemplos negativos asociados \textit{(Un, saltar, 0), (Un, violeta, 0)}. De este modo, se tendrían cuatro palabras: la entrada \textit{Un} y \textit{coche, saltar, violeta} como contexto. En el siguiente paso, se busca la palabra de entrada en la matriz de \textit{embedding} y el resto en la matriz de contexto. A continuación, se realiza el producto escalar para los 3 vectores y se compara el resultado con las etiquetas \textit{(1, 0, 0)} para este caso. En la primera iteración, al estar inicializadas las matrices con valores aleatorios, los resultados no coincidirán con las etiquetas, por tanto, es necesario actualizar las matrices de \textit{embedding} y contexto según el error cometido. Para este proceso, se utiliza el método de descenso de gradiente estocático \cite{Bengio2003}. Este proceso completo se repite para todos los patrones de entrada durante un número de iteraciones que, como resultado, generará la matriz de \textit{embedding} con los vectores de números reales asociados a cada término del vocabulario.

La matriz de \textit{embedding} resultante del proceso de entrenamiento contendrá, por tanto, un vector numérico para cada término que exista dentro del vocabulario. Por ejemplo, si existiera la palabra ``banco'' dentro del corpus de entrenamiento, esta matriz contendría un vector numérico que identificaría esta palabra y su relación semántica con el resto de términos que conforman el corpus. Sin embargo, debido a la ambigüedad del lenguaje, este término puede tener más de un significado dependiendo del contexto, por ejemplo, se podría encontrar el término dentro de los siguientes documentos:

	\textit{``He ido al banco a ingresar dinero''}
	
	\textit{``Me he sentado en el banco del parque''}
	
Pese a que el significado del término es distinto, el algoritmo \textit{word2vec} representaría la palabra con el mismo vector. Para abordar este problema, surgen los métodos de \textit{word-embeddings} contextualizados. En este tipo de métodos, en lugar de utilizar un vector fijo para cada término, se observa el contexto en el que se utiliza para asignar un vector con el que codificar la palabra. Dentro de estos métodos destacan ELMo \cite{Matthew2018} y BERT \cite{Jacob2019} que ha alcanzado los mejores resultados hasta la fecha para numerosas tareas relacionadas con el procesado del lenguaje.


\subsection{Clasificación}
\label{sec:Ejemplo_subSeccion} 
\noindent Como ya se introdujo en apartados anteriores, la clasificación automática de documentos se puede entender como aquella tarea en la que un documento, o una parte del mismo, es etiquetado como perteneciente a un determinado conjunto, grupo o categoría predeterminada.

Los métodos de clasificación supervisados utilizan un conjunto de documentos de ejemplo para cada una de las categorías que presenta la variable objetivo (a clasificar). Estos algoritmos, realizan una etapa de entrenamiento donde se presentan los patrones de ejemplo de modo que ante futuros patrones, el algoritmo será capaz de clasificar en alguna de las clases contenidas en el conjunto de ejemplo. Dentro de este proceso, existen muchas variables que influirán en los resultados del sistema como el tamaño del conjunto de ejemplo, la elección del algoritmo de clasificación o los parámetros de inicialización del mismo.

Existen numerosos tipos de algoritmos de clasificación, a continuación se indican los más importantes para clasificación textual:

\begin{itemize}
	\itemsep0em 
	\item Naive Bayes \cite[Capitulo~3]{Jurafsky}: Está basado en la teoría de la decisión de Bayes: la teoría de las probabilidades condicionadas. Por tanto, el problema de la clasificación se reduce al cálculo de las probabilidades a posteriori de una clase dado un documento.
	\item Arboles de decisión \cite{Leo2001}: Se trata de un método que a través de un proceso recursivo de las los atributos de entrada, realiza una representación para clasificar el conjunto de datos presentado.
	\item Máquinas de vectores de soporte (``Support Vector Machine'', SVM) \cite{Corinna1995}: Estos algoritmos pretenden encontrar una hipersuperficie de separación entre clases dentro del espacio de representación.
	\item Redes Neuronales \cite{Goodfellow-et-al-2016}: Son un modelo computacional compuesto por elementos ("neuronas") interconectados entre sí que aplican una transformación a los datos para producir una salida. Es posible entrenar una red neuronal para que dada una entrada determinada (un vector de representación) produzca una salida deseada (la categoría a la que corresponde ese documento). Dentro de este tipo de algoritmos, destacan las ``redes neuronales profundas'' (\textit{deep learning}) que proveen de una herramienta muy poderosa añadiendo más capas de neuronas que permiten representar funciones de mayor complejidad. Este tipo de técnicas alcanza los mejores resultados hasta la fecha en tareas como el procesado de imagen \cite{Mingxing2019} o el procesado de textos \cite{Jacob2019}.
	\item KNN (K-Nearest Neighbour) \cite{Gongde2003}: Este algoritmo se basa en la aplicación de una métrica que establezca la similitud entre un documento que se quiere clasificar y cada uno de los documentos de entrenamiento. La clase o categoría que se asigna al documento sería la categoría del documento más cercano según la métrica establecida.
\end{itemize}

%Random forest (Breiman, 2001) is an ensemble of unpruned classification or regression trees, induced from
%bootstrap samples of the training data, using random feature selection in the tree induction process. Prediction is made by aggregating (majority vote for classification or averaging for regression) the predictions of
%the ensemble. Random forest generally exhibits a substantial performance improvement over the single tree
%classifier such as CART and C4.5. It yields generalization error rate that compares favorably to Adaboost,
%yet is more robust to noise. However, similar to most classifiers, RF can also suffer from the curse of learning from an extremely imbalanced training data set. As it is constructed to minimize the overall error rate, it
%will tend to focus more on the prediction accuracy of the majority class, which often results in poor accuracy
%for the minority class. To alleviate the problem, we propose two solutions: balanced random forest (BRF)
%and weighted random forest (WRF).

\section{Detección de lenguaje o discurso del odio (\textbf{\textit{hate speech detection}}) }
\label{sec:Ejemplo_seccion}
La detección del lenguaje machista o sexista está muy relacionada con la detección del lenguaje o discurso del odio en redes sociales. Existen numerosos trabajos donde se intenta detectar distintos tipos de lenguaje del odio, entre ellos el sexismo \cite{Watanabe2018,WaseemHovy2016,Georgios2018,Badjatiya2017,Zimmerman2018,Park2017,Waseem2016}. El lenguaje del odio se refiere al uso de lenguaje agresivo, violento u ofensivo hacia un grupo específico de personas que comparten una propiedad en común, sea esta propiedad su género, su raza, sus creencias o su religión \cite{Davidson2017}. Atendiendo a esta definición, se puede considerar la detección del machismo como un caso particular del discurso del odio. Por ello, es muy interesante realizar una evaluación de los trabajos realizados en esta línea de investigación.

La detección del lenguaje del odio es una linea de investigación muy actual, datando el primer estudio evaluado en el año 2012 \cite{Xiang2012}. En este articulo se emplea un modelo de detección de temas o categorías (\textit{topic modelling}) que explota la concurrencia de palabras para la creación de atributos o \textit{features} que alimentarán un algoritmo de clasificación de aprendizaje de máquina o \textit{machine learning}. En la mayoría de trabajos previos se empleaban soluciones basadas en patrones para la clasificación de tweets. Utilizando estos métodos, el uso de expresiones coloquiales y soeces en redes sociales hace más complicado establecer las fronteras entre el uso de lenguaje ofensivo que no tiene como objetivo despreciar a ningún grupo de personas y el lenguaje del odio \cite{Davidson2017}. De este modo, este artículo supone un paso muy importante hacia la automatización y a los sistemas basados en algoritmos de \textit{machine learning}.

Durante los últimos tres años, se han sucedido diferentes artículos en la temática aumentando considerablemente la producción científica en este campo. En \cite{WaseemHovy2016} se aporta el primer corpus de referencia anotado que se utilizará posteriormente en \cite{Waseem2016,Georgios2018,Badjatiya2017,Zimmerman2018,Park2017}. Está compuesto por 16.000 \textit{tweets} etiquetados en mensajes sexistas, racistas o sin contenido ofensivo. En este primer trabajo, se sientan las bases de las soluciones aplicadas en el resto de artículos, se utilizan atributos como los \textit{unigramas, bigramas, trigramas} y \textit{cuatri-gramas} y un algoritmo de regresión logística para la clasificación.

En el artículo desarrollado por el mismo autor \cite{Waseem2016} se propone una solución similar pero se amplía el corpus en 4033 \textit{tweets} y se utiliza una plataforma de \textit{crowdsourcing} para anotar los mensajes, lo que introduce más diversidad en los criterios del etiquetado. Según los autores, el empeoramiento de los resultados puede deberse al posible sesgo que se produce en \cite{WaseemHovy2016}, ya que los \textit{tweets} fueron etiquetados por los autores únicamente.

En el resto de artículos que evalúan su propuesta utilizando el corpus desarrollado por \cite{Waseem2016}, se utilizan redes neuronales en la etapa de clasificación y, en algunos, en la etapa de preprocesamiento. En la solución propuesta por \cite{Zimmerman2018} se aplican redes neuronales convolucionales (\textit{CNN, Convolutional Neural Network}) para codificar el texto y extraer los atributos que se utilizarán para el clasificador final, basado también en CNNs. Esta técnica permite tener en cuenta la posición de la palabra (su contexto) para extraer los atributos de cada \textit{tweet}. Esta misma idea junto con el uso de redes neuronales recurrentes (\textit{RNN, Recurrent Neural Network}) se utiliza en \cite{Badjatiya2017} para obtener los atributos en la etapa de procesamiento. En ambos artículos se consiguen mejorar los resultados alcanzados por \cite{Waseem2016} lo que afianza el uso de técnicas basadas en redes neuronales en el procesado del lenguaje.

Una idea interesante es el uso de atributos como la tendencia al racismo o al sexismo sirviéndose del historial de los usuarios. En \cite{Georgios2018} se demuestra como el uso de este tipo de atributos mejora notablemente los resultados. Esta misma idea se utiliza en \cite{Chatzakouy2017} donde se detectan cuentas agresivas estudiando al usuario y su red de seguidores.

En todos los artículos revisados anteriormente, se trata el problema como una clasificación múltiple donde el texto se puede clasificar según las etiquetas racismo, sexismo o ninguno. Sin embargo, se podría resolver el problema con un doble clasificador, el primero detecta si el texto contiene lenguaje abusivo o no y el segundo realizaría la tarea de clasificar en contenido sexista o racista \cite{Park2017}.

Un desafío importante en la detección del lenguaje del odio en redes sociales es la separación entre el lenguaje ofensivo y el lenguaje que incita o promueve el odio. Davidson \cite{Davidson2017} aporta un corpus etiquetado de 25.000 \textit{tweets} para diferenciar entre estos 2 tipos de lenguaje. En su trabajo, se propone un modelo similar a \cite{Waseem2016} donde se ponen de manifiesto las dificultades de esta solución para considerar el contexto de las palabras. De este modo, si se utilizan palabras que pueden expresar odio (por ejemplo, "\textit{gay}") en un contexto positivo, hay muchas probabilidades de que el sistema detecte odio en el texto. Los resultados serán mejorados posteriormente en \cite{Watanabe2018} donde se ampliará el número de \textit{features} y se utilizará un algoritmo basado en árboles de decisión para la tarea de clasificación.


\section{Detección de la misoginia}
\label{sec:Ejemplo_seccion}
La misoginia se define según la RAE como \textit{``Aversión a las mujeres''} \cite{MisoginiaRAE}. El machismo, sin embargo, se define como ``Actitud de prepotencia de los varones respecto de las mujeres'' o ``forma de sexismo caracterizada por la prevalencia del varón'' \cite{MachismoRAE}. Si bien estos dos términos tienen matices distintos, tienen como denominador común la discriminación de las mujeres debido a su sexo. De hecho, existen trabajos donde se expone que la misoginia se manifiesta lingüísticamente mediante la exclusión, discriminación, hostilidad, trato de violencia objetificación o cosificación sexual \cite{Anzovino2018,Fersini2018}. Muchas de estas señales textuales de misoginia serían aplicables del mismo modo al machismo \cite{Garazi2014,Giraldo1972}. 

Durante este último año, se ha llevado a cabo la competición IberEval 2018 donde una de las tareas era la detección automática de la misoginia \cite{AMI2018} (AMI, \textit{``Automatic Misogyny Identification''}). En esta tarea se propone la labor de identificar la misoginia en \textit{tweets} en español e inglés. En total, participaron once equipos de cinco países distintos para la detección en inglés, mientras que para la detección en castellano participaron un total de ocho equipos \cite{AMIOverview2018}. Los artículos publicados para esta tarea en castellano resultan de gran interés, pues guarda una relación importante con el presente trabajo.

Para la tarea de clasificación, la mayoría de los equipos utilizaron Máquinas de Vectores de Soporte (SVM, \textit{Suppor Vector Machines}) y métodos combinados de aprendizaje (EoC, \textit{Ensemble of Classifiers}). Las técnicas basadas en SVMs fueron utilizadas por \cite{Canos2018,Wahyu2018,Victor2018} mientras que los equipos \cite{Ahluwalia2018,Shushkevich2018,Frenda2018,Liu2018} aplicaron técnicas EoC.

Las soluciones aportadas por \cite{Canos2018,Wahyu2018} obtuvieron la mejor tasa de acierto para la detección de la misoginia en castellano. El modelo propuesto por \cite{Canos2018} utiliza \textit{features} basadas en la vectorización de cada tweet, utilizando la medida tf-idf (\textit{term frequency - Inverse document frequency}). Posteriormente, se emplea un modelo SVM con núcleo lineal para la etapa de clasificación. Esta solución tan sencilla alcanza los mejores resultados para \textit{tweets} en castellano, pero empeora considerablemente para \textit{tweets} en inglés.

Una idea interesante, explorada en \cite{Wahyu2018}, es el uso de un léxico auxiliar que contenga palabras que se encuentren con frecuencia en textos sexistas. Este léxico fue desarrollado en un trabajo italiano \cite{Mauro2016}. En dicho estudio, se utiliza como clasificador un modelo basado en SVM con núcleo lineal para el castellano y núcleo radial para el inglés. En este caso, se alcanza la máxima tasa de acierto en inglés y en español.

\cite{Goenaga2018} fue uno de los pocos trabajos donde se exploraron soluciones basadas en redes neuronales. En este trabajo se utilizan redes neuronales recurrentes (\textit{Recurrent Neural Network, RNN}) como sistema de clasificación realizando previamente un preprocesado basado en \textit{word embeddings} que permite codificar las palabras o términos mediante números reales. Pese a que este tipo de técnicas han mostrado su eficacia en tareas relacionadas con el procesado de texto \cite{Jacob2019}, esta solución queda lejos de los mejores resultados alcanzados en la competición.

\subsection{Corpus disponibles}
\label{sec:Ejemplo_subSeccion} 
A continuación se citan algunos corpus que pueden ser utilizados para la detección de lenguaje del odio en textos:
\begin{itemize}
	\itemsep0em 
	\item IberEval 2018 Automatic Misogyny Identification \cite{AMIOverview2018}: Se trata de un corpus etiquetado que contiene campos que denotan si el texto contenido en un tweet tiene un componente sexista. Fue recogido entre el 20-07-2018 y 30-11-2017 donde se recogieron 83 millones de tweets en inglés y 72 millones en castellano. Para el proceso de etiquetado se utilizadon dos pasos: en el primero dos anotadores etiquetaban el conjunto y en el segundo se utilizó una plataforma de crowdsourcing. Finalmente, se etiquetaron 3521 tweets en inglés y 3307 en español para la fase de entrenamiento. En cuanto al conjunto de test, se compartieron 831 tweets en español y 726 en inglés.
	\item Corpus etiquetado \cite{WaseemHovy2016}: Está compuesto por \textit{tweets} etiquetados para mensajes sexistas, racistas o sin contenido ofensivo. Se trata de un conjunto de datos recolectado durante dos meses y compuesto por 136.052 de los cuales se etiquetaron 16.614. De los tweets etiquetados, 3.383 contienen mensajes machistas, 1972 contienen expresiones racistas y  11.559 mensajes libres de contenido ofensivo. 
\end{itemize}
 
